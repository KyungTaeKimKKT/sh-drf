from bs4 import BeautifulSoup
import concurrent.futures
import requests
from django.db import transaction
from urllib.parse import urlparse
from urllib.request import urlopen
import datetime
import time, json, copy
from dateutil import parser

import scraping.models as ScrapingModel
from util.utils_func import remove_duplicates_from_model

import logging, os, traceback
### ì›ì¹™ì€ __name__ì´ë‚˜, í˜„ì¬ appì˜ í•˜ìœ„ í´ë”ì´ë¯€ë¡œ 
# logger = logging.getLogger(__name__)
logger = logging.getLogger('scheduler_job')
print ( f" ë¡œê±° ì„¤ì • : {logger}")
print ('__name__', __name__)


class ì •ë¶€ê¸°ê´€NEWS_Scraping:
	""" ì •ë¶€ê¸°ê´€ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ 
		config :  dict :{'url': 'https://www.moel.go.kr/news/enews/report/enewsList.do', 
						'gov_name': 'ê³ ìš©ë…¸ë™ë¶€', 
						'êµ¬ë¶„': 'ë³´ë„ìë£Œ', 
						'suffix_link': 'https://www.moel.go.kr/news/enews/report/'}
		th_db :  dict  : {'ì œëª©': 'ì œëª©', 'ë“±ë¡ì¼': 'ë“±ë¡ì¼', 'ì‘ì„±ì¼': 'ë“±ë¡ì¼', 
					'ë“±ë¡ì¼ì': 'ë“±ë¡ì¼', 'ë‚ ì§œ': 'ë“±ë¡ì¼', 
					'ì¥í•™ìƒ': 'ì œëª©ê¸ˆì§€ì–´', 'ë°•ì¢…ì„ ': 'ì œëª©ê¸ˆì§€ì–´'}
	"""
	def __init__(self, config:dict={}, th_db:dict={}, db_attributes:list[str]=[]):
		self.gov_name: str = ''
		self.êµ¬ë¶„: str = ''
		self.url: str = ''
		self.link_suffix: str = ''
		self.db_attributes: list[str] = db_attributes
		self.ths, self.tds, self.results = [], [], []

		self.th_db = th_db
		self.config = config
		for key, value in self.config.items():
			setattr(self, key, value)

		self.link_testResult = False

		self.defaultDict = {
			'ì •ë¶€ê¸°ê´€':self.gov_name,
			'êµ¬ë¶„' : self.êµ¬ë¶„,
		}
		self.result = {}
		self.log = {}
		self.errorList = []
		self.timeout = 5.0
		self.soup = self.get_soup( self.url )
		self.run()

	def run(self) -> list[dict]:
		if self.soup is not None:
			match self.gov_name:
				# case 'í•œêµ­ì†Œë°©ì•ˆì „ì›':		
				# 	ul = self.soup.find( 'ul', {'class':'orderlist'})
				# 	lis = ul.find_all('li')
				# 	for (index,li) in enumerate(lis):
				# 		li : BeautifulSoup
				# 		if ( title:= li.find('p', {'class':'title'}) ):
				# 			title_text = title.text.strip().replace( title.find('span').text, "") 
				# 			_isValid, ë“±ë¡ì¼ = self.check_ë“±ë¡ì¼_ê²€ì¦(ë“±ë¡ì¼ = li.find_all('span')[2].text.strip() )
				# 			if _isValid :
				# 				link = self.link_suffix + self.get_href( a = li.find('a', href=True) )
				# 				updateDict = copy.deepcopy( self.defaultDict )
				# 				updateDict.update(
				# 					{
				# 						'ì œëª©':title_text,
				# 						'ë“±ë¡ì¼' : ë“±ë¡ì¼,
				# 						'ë§í¬' : link,
				# 					} 
				# 				)
				# 				self.results.append (updateDict)

				# 				if index == 0:
				# 					self.check_link_test(link)							

				case _:
					self.ths:list[str] = self.get_tableThs()
					result_list = self.get_tableTds()
					self.results = self.validate_result(result_list)

		return self.results

	# ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ íŒŒì‹±í•œ í›„ date() ë©”ì„œë“œë¡œ ë‚ ì§œë§Œ ì¶”ì¶œ
	def parse_to_date(self, date_string:str) -> datetime.date|None:
		try:
			# ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ íŒŒì‹±
			dt = parser.parse(date_string)
			# datetimeì—ì„œ date ê°ì²´ë§Œ ì¶”ì¶œ
			return dt.date()
		except Exception as e:
			logger.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
			return None
		
	def validate_result(self, result_list:list[dict]) -> list[dict]:
		""" ê²€ì¦ ê²°ê³¼ ë°˜í™˜ """

		ë‚ ì§œ_str = None
		for _th_str in self.ths:
			match self.th_db.get(_th_str, None):
				case 'ì œëª©':
					pass
				case 'ë“±ë¡ì¼':
					ë‚ ì§œ_str = _th_str
				case _:
					pass

		results = []
		for _obj in result_list:
			# ìƒˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•˜ì—¬ ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ë¥¼ ë³µì‚¬
			new_obj = copy.deepcopy(self.defaultDict)
			
			for key, value in _obj.items():
				if key in self.db_attributes:
					if key == ë‚ ì§œ_str:
						new_obj['ë“±ë¡ì¼'] = self.parse_to_date(value)
					else:
						new_obj[key] = value
					
			results.append(new_obj)
		return results

	
	def check_link_test(self, link:str) -> bool:
		### ğŸ˜€ https://stackoverflow.com/questions/56101612/python-requests-http-response-406
		res = requests.get( link, timeout= self.timeout, headers={"User-Agent": "XY"})
		self.link_testResult = res.ok
		if not self.link_testResult:
			self.errorList.append(f"  {link} test Falil : {res.status_code} -- {self.defaultDict['ì •ë¶€ê¸°ê´€']} - {self.defaultDict['êµ¬ë¶„']}")
		return res.ok
	

	def get_soup(self, url) -> BeautifulSoup|None :
		if not url : return None
		try:
			page = urlopen(url, timeout= self.timeout)
			html:str = page.read().decode("utf-8")
			soup = BeautifulSoup(html, features="html.parser")
			return soup
		except Exception as e:
			self.errorList.append(str(e))
			self.log['url'] = self.errorList
			return None
		

	def get_tableThs(self, soup:BeautifulSoup|None=None) ->list[str]:
		""" get table theads """
		if not soup : 
			soup =self.soup
		webìƒ_ths = [ th.text.strip() for th in soup.find_all('th') ]
		dbìƒ_ths = []
		for th_name in webìƒ_ths:
			if th_name in self.th_db:
				dbìƒ_ths.append( self.th_db[th_name] )
			else:
				dbìƒ_ths.append( th_name )

		return dbìƒ_ths  
	
	def get_tableTds(self, soup:BeautifulSoup|None=None ) -> list[dict[str, str]]:
		if not soup : soup =self.soup
		result_list =[]
		try:
			table = soup.find('table')
			table_body = table.find('tbody')
			table_trs = table_body.find_all('tr')
			
			for tr in table_trs:
				tds_dict = {}
				for th, td in zip(self.ths, tr.find_all('td')):
					# ëª¨ë“  ê³µë°±, íƒ­, ê°œí–‰ ë¬¸ì ë“±ì„ ì œê±°
					cleaned_text = td.text.strip().replace('\r', '').replace('\n', '').replace('\t', '')
					# ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
					cleaned_text = ' '.join(cleaned_text.split())
					tds_dict[th] = cleaned_text
					tds_dict['ë§í¬'] = self.get_href( a = tr.find('a', href=True) )
				result_list.append( tds_dict )
			
			return result_list
		except Exception as e:
			self.errorList.append(f" Anayalis Error: {e}")
			return []
		# global scraping_results
		# scraping_results.append({self.url : result })
		# return result
	
	def get_href(self, a:BeautifulSoup|None=None) -> str:
		if a is None : return '#'
		try:
			#### onclick ê²½ìš°, 
			if a['href'] == '#':
				###  goTo.view('list','13931','134','0402010000'); return false;
				if a['onclick'] is not None and len(a['onclick']) > 5:
					match self.config.get('gov_name').strip() :
						case 'í•œêµ­ìŠ¹ê°•ê¸°ì•ˆì „ê³µë‹¨':
							link1 = a['onclick'].replace('(', '__').replace(')', '__').split('__')[1]
							link_list = link1.split(',')
							mId = link_list[3].strip("'")
							bIdx = link_list[1].strip("'")
							ptIdx = link_list[2].strip("'")
							return f"mId={mId}&bIdx={bIdx}&ptIdx={ptIdx}"
						
						case 'í•œêµ­ì†Œë°©ì•ˆì „ì›':
							link1 = a['onclick'].replace('(', '__').replace(')', '__').split('__')[1]
							link_list = link1.split(',')
							postsSeqno = link_list[0].strip("'")
							postsRnum = link_list[1].strip("'")

							return f"&postsSeqno={postsSeqno}&postsRnum={postsRnum}&searchCondition=title&searchKeyword=&pageIndex=1"
						
						case '_':
							return '#'
				else:
					return '#'

			elif ';jsessionid=' in a['href']:
				linkList = a['href'].split(';')
				return linkList[0]+'?'+linkList[1].split('?')[-1]
				ì¡°ì •ëœlink = linkList[1].split('?')
			else:
				#### ëŒ€í•œ ì‚°ì—…ì•ˆì „í˜‘íšŒ ê²½ìš° https://www.safety.or.kr/
				#### /safety/bbs/BMSR00207/view.do;jsessionid=E183C56293052018F959EDB82EFBF652?boardId=229015&menuNo=200082&pageIndex=1&searchCondition=&searchKeyword=
				### ì—ì„œ ;jsessionid=E183C56293052018F959EDB82EFBF652 ì‚­ì œí•¨
				
				return a['href'] if bool(a['href'])  else '#'
		except Exception as e:
			self.errorList.append(f"href find error:{e}")
			return '#'

def get_new_articles_from_db_compare( today_results:list[dict] ) -> tuple[int, list[dict]]:
	""" ì˜¤ëŠ˜ ìŠ¤í¬ë˜í•‘ëœ í•­ëª©ê³¼ DBì— ìˆëŠ” í•­ëª©ì„ ë¹„êµí•˜ì—¬ tuple ( ì‹ ê·œ í•­ëª© ë°˜í™˜ , ì‚­ì œëœ í•­ëª© ë°˜í™˜ ) """
	today = datetime.datetime.now().date()
	ë¹„êµëŒ€ìƒ_key_values = ['ì •ë¶€ê¸°ê´€', 'êµ¬ë¶„', 'ì œëª©', 'ë§í¬']
	today_db_values = ScrapingModel.NEWS_DB.objects.filter( ë“±ë¡ì¼ = today ).values(*ë¹„êµëŒ€ìƒ_key_values)

	logger.info( f" ì˜¤ëŠ˜ DBì— ìˆëŠ” í•­ëª©: {len(today_db_values)}ê°œ")
	# DBì— ì—†ëŠ” ì‹ ê·œ í•­ëª© ì°¾ê¸°
	new_articles, del_articles = [], []
	
	# DB í•­ëª©ì„ ë¹„êµí•˜ê¸° ì‰½ê²Œ ì„¸íŠ¸ë¡œ ë³€í™˜
	db_items_set = set()
	for db_item in today_db_values:
		# ë”•ì…”ë„ˆë¦¬ëŠ” í•´ì‹œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ íŠœí”Œë¡œ ë³€í™˜
		item_tuple = tuple((k, str(v).strip()) for k, v in db_item.items() if k in ë¹„êµëŒ€ìƒ_key_values)
		db_items_set.add(item_tuple)
	
	# ìŠ¤í¬ë˜í•‘ ê²°ê³¼ì—ì„œ DBì— ì—†ëŠ” í•­ëª© ì°¾ê¸°
	for result_item in today_results:
		# ë¹„êµë¥¼ ìœ„í•´ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
		item_tuple = tuple((k, str(result_item.get(k, '').strip())) for k in ë¹„êµëŒ€ìƒ_key_values 
					 if not isinstance ( result_item.get(k, ''), (datetime.date, datetime.datetime)))
		
		# DBì— ì—†ëŠ” í•­ëª©ì´ë©´ ì‹ ê·œ í•­ëª© ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
		if item_tuple not in db_items_set:
			new_articles.append(result_item)
		else:
			del_articles.append(result_item)

	logger.info( f" ì‹ ê·œ í•­ëª©: {len(new_articles)}ê°œ")
	logger.info( f" ì‚­ì œ í•­ëª©: {len(del_articles)}ê°œ")

	return new_articles, del_articles


def update_urls ( new_articles:list[dict] , url_prefix_dict:dict ) -> list[dict]:
	""" ì‹ ê·œ articleì— ëŒ€í•œ url ì ‘ì†ì—¬ë¶€ ê²€ì¦ """
	for _obj in new_articles:
		if _obj and _obj.get('ì •ë¶€ê¸°ê´€', '') and _obj.get('êµ¬ë¶„', ''):
			key = _obj.get('ì •ë¶€ê¸°ê´€')+'_'+_obj.get('êµ¬ë¶„')
			prefix = url_prefix_dict.get(key)
			if prefix is None:
				logger.error(f"{_obj.get('ì •ë¶€ê¸°ê´€')} - {_obj.get('êµ¬ë¶„')} : URL ì ‘ë‘ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
			else:
				_obj['ë§í¬'] = prefix + _obj.get('ë§í¬')
	return new_articles

def ê²€ì¦_urls_ì ‘ì†ì—¬ë¶€( new_articles:list[dict]  ) -> list[dict]:
	""" ì‹ ê·œ articleì— ëŒ€í•œ url ì ‘ì†ì—¬ë¶€ ê²€ì¦ """

	for _obj in new_articles:
		# _obj['ì ‘ì†ì—¬ë¶€'] = False
		url = _obj.get('ë§í¬').strip()
		if url.startswith('http') or url.startswith('https'):
			try:
				# GET ìš”ì²­ìœ¼ë¡œ ë³€ê²½ (HEAD ìš”ì²­ì´ ê±°ë¶€ë˜ëŠ” ì‚¬ì´íŠ¸ê°€ ìˆìŒ)
				# í—¤ë” ì¶”ê°€ ë° User-Agent ê°œì„ 
				headers = {
					"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
					"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
					"Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
					"Connection": "keep-alive"
				}
				
				# HEAD ìš”ì²­ ëŒ€ì‹  GET ìš”ì²­ ì‚¬ìš© (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” HEAD ìš”ì²­ì„ ê±°ë¶€í•¨)
				response = requests.get(url, timeout=5, allow_redirects=True, headers=headers, stream=True)
				
				# ìŠ¤íŠ¸ë¦¼ ëª¨ë“œë¡œ ìš”ì²­í•˜ê³  ì¼ë¶€ ì½˜í…ì¸ ë§Œ ì½ì–´ì„œ ê²€ì¦ (ì „ì²´ í˜ì´ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ)
				response.raw.read(1024)
				response.close()
				
				# ìƒíƒœ ì½”ë“œê°€ 200ëŒ€ë©´ ì„±ê³µ
				if 200 <= response.status_code < 300:
					# _obj['ì ‘ì†ì—¬ë¶€'] = True
					logger.info(f"{_obj.get('ì •ë¶€ê¸°ê´€')} - {_obj.get('êµ¬ë¶„')} : URL ì ‘ì† ì„±ê³µ: {url}")
				else:
					logger.error(f"{_obj.get('ì •ë¶€ê¸°ê´€')} - {_obj.get('êµ¬ë¶„')} : URL ì ‘ì† ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ {response.status_code}): {url}")
					# _obj['ì ‘ì†ì—¬ë¶€'] = False
					
			except requests.RequestException as e:
				logger.error(f"{_obj.get('ì •ë¶€ê¸°ê´€')} - {_obj.get('êµ¬ë¶„')} : URL ì ‘ì† ì˜¤ë¥˜: {url} - {str(e)}")
		else:
			logger.error(f"{_obj.get('ì •ë¶€ê¸°ê´€')} - {_obj.get('êµ¬ë¶„')} : URL ê²€ì¦ ì‹¤íŒ¨ (httpë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ): {url}")

	return new_articles



def main_job(job_id:int):
	import time
	s = time.time()
	try:
		#### 1. dbì—ì„œ ê¸°ë³¸ì •ë³´ ê°€ì ¸ì˜´. : ì •ë¶€ê¸°ê´€_DB, NEWS_Table_Head_DB
		ì •ë¶€ê¸°ê´€_DB_QS = ScrapingModel.ì •ë¶€ê¸°ê´€_DB.objects.all().values('url', 'gov_name', 'êµ¬ë¶„','suffix_link')
		ì •ë¶€ê¸°ê´€_DB_list = [{k: v.strip() if isinstance(v, str) else v for k, v in obj.items()} for obj in ì •ë¶€ê¸°ê´€_DB_QS]
		NEWS_Table_Head_DB_QS = ScrapingModel.NEWS_Table_Head_DB.objects.all().values('ì œëª©','ë“±ë¡ì¼','ì œëª©ê¸ˆì§€ì–´')
		# NEWS_Table_Head_DB_list = [{k: v.split(',') if isinstance(v, str) else v for k, v in obj.items()} for obj in NEWS_Table_Head_DB_QS]
		#### reverse dict
		NEWS_Table_Head_DB_dict = {value: key for item in NEWS_Table_Head_DB_QS for key, values in item.items() for value in values.split(',')}

		db_attributes = ['ì •ë¶€ê¸°ê´€', 'êµ¬ë¶„', 'ì œëª©', 'ë“±ë¡ì¼', 'ë§í¬']
		url_prefix_dict = { _obj.get('gov_name')+'_'+_obj.get('êµ¬ë¶„') : _obj.get('suffix_link') for _obj in ì •ë¶€ê¸°ê´€_DB_list}

		if not ì •ë¶€ê¸°ê´€_DB_list :
			raise ValueError ( "ì •ë¶€ê¸°ê´€_DB_list ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤." )
		if not NEWS_Table_Head_DB_dict :
			raise ValueError ( "NEWS_Table_Head_DB_dict ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤." )
		
		threadingTargets = ì •ë¶€ê¸°ê´€_DB_QS

		# https://stackoverflow.com/questions/6893968/how-to-get-the-return-value-from-a-thread
		### 2. ìŠ¤í¬ë˜í•‘ ì‹œì‘
		with concurrent.futures.ThreadPoolExecutor() as executor:
			futures = [ executor.submit (ì •ë¶€ê¸°ê´€NEWS_Scraping, config , NEWS_Table_Head_DB_dict , db_attributes ) 
						for config in threadingTargets ]
		result_Collections = [f.result() for f in futures]
		
		### 3. ìŠ¤í¬ë˜í•‘ ì™„ë£Œ
		all_results =[]
		for scraping_instance in result_Collections:
			if scraping_instance.results:
				all_results.extend( scraping_instance.results )

		logger.info( f" ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(all_results)}ê°œ")

		### 4. ì˜¤ëŠ˜ ìŠ¤í¬ë˜í•‘ëœ í•­ëª© ì¶”ì¶œ

		for _obj in all_results:
			logger.info( { k: v for k, v in _obj.items() if k not in ['ë§í¬'] } )
		today = datetime.datetime.now().date()
		today_results = [ result for result in all_results if result.get('ë“±ë¡ì¼') == today ]
		### 5. ë§í¬ ì—…ë°ì´íŠ¸
		today_results = update_urls( today_results , url_prefix_dict )
		logger.info( f" ì˜¤ëŠ˜ì ìŠ¤í¬ë˜í•‘ëœ í•­ëª©: {len(today_results)}ê°œ")
		### 6. ì˜¤ëŠ˜ ìŠ¤í¬ë˜í•‘ëœ í•­ëª©ê³¼ DBì— ìˆëŠ” í•­ëª©ì„ ë¹„êµí•˜ì—¬ ì‹ ê·œ í•­ëª© ë°˜í™˜
		new_articles , del_articles = get_new_articles_from_db_compare( today_results )
		logger.info( f" dbì— ì—†ëŠ” ì‹ ê·œ í•­ëª©: {len(new_articles)}ê°œ")
		logger.info( f" dbì— ìˆëŠ” ì‚­ì œ í•­ëª©: {len(del_articles)}ê°œ")
		### 7. ì‹ ê·œ í•­ëª© url ê²€ì¦
	
		logger.info( f'url ê²€ì¦ ì‹œì‘: {len(new_articles)}ê°œ')

		new_articles = ê²€ì¦_urls_ì ‘ì†ì—¬ë¶€( new_articles )

		### 8. ê²€ì¦ ì™„ë£Œ
		log = {}
		log['ì˜¤ëŠ˜ ìŠ¤í¬ë˜í•‘ëœ í•­ëª©'] = len(today_results)
		log['DBì— ì´ë¯¸ ìˆëŠ” í•­ëª©'] = len(del_articles)
		log['ì‹ ê·œ í•­ëª©'] = len(new_articles)
		logger.info( f'ì™„ë£Œ ë° db ì €ì¥ ì‹œì‘: {log}')

		### 9. ì‹ ê·œ í•­ëª© DBì— ì €ì¥
		if new_articles:
			db_save_list = [ {k: v for k, v in x.items() if k in db_attributes} for x in new_articles ]

			bulk_list = [ ScrapingModel.NEWS_DB(**article) for article in db_save_list]
			with transaction.atomic():
				ScrapingModel.NEWS_DB.objects.bulk_create(bulk_list)

			### 9. delete dubplicate í•­ëª© DBì—ì„œ ì‚­ì œ
			duplicate_counts = remove_duplicates_from_model( ScrapingModel.NEWS_DB, db_attributes )
			logger.info( f" ì¤‘ë³µ í•­ëª© ì‚­ì œ ì™„ë£Œ: {duplicate_counts}ê°œ")

		logger.info( f" ì†Œìš”ì‹œê°„ : {time.time()-s} ì´ˆ")
		return {
				'log' : log,
				'redis_publish' : new_articles,
			}

	except ValueError as e:
		logger.error( 'ValueError :%s', str(e) ,exc_info=True)
		return {
			'log' : {
				'error' : str(e),			
			}
		}
	except Exception as e:
		logger.error( 'Exception :%s', str(e) )
		logger.error ( f" ìƒì„¸ì˜¤ë¥˜ ì •ë³´: \n{traceback.format_exc()}")
		return {
			'log' : {
				'error' : str(e),			
			}
		}





if __name__ == '__main__':
	return_value = main_job()
	# logger.info( f" ì™„ë£Œ ë° db ì €ì¥ ì™„ë£Œ: {return_value}")
# 	# asyncio.run(main() )
# 	ws = WS(Info.URL_WS_ì •ë¶€ê¸°ê´€NEWS_STATUS)
# 	ws.run()

# 	main(ws)

# 	# # scheduler = BackgroundScheduler()
# 	scheduler = BlockingScheduler(job_defaults={'max_instances': 3})
# 	scheduler.add_job( main, trigger='cron',hour='6-23', minute='0,30' , args=[ws], id='main' )
# 	scheduler.start()